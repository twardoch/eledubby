Project Structure:
ğŸ“ eledubby
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 301.txt
â”œâ”€â”€ ğŸ“ src
â”‚   â””â”€â”€ ğŸ“ eledubby
â”‚       â”œâ”€â”€ ğŸ“ api
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ elevenlabs_client.py
â”‚       â”œâ”€â”€ ğŸ“ audio
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ analyzer.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ extractor.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ processor.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ segmenter.py
â”‚       â”œâ”€â”€ ğŸ“ utils
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ progress.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ temp_manager.py
â”‚       â”œâ”€â”€ ğŸ“ video
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ remuxer.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ __main__.py
â”‚       â”œâ”€â”€ ğŸ“„ config.toml
â”‚       â””â”€â”€ ğŸ“„ eledubby.py
â”œâ”€â”€ ğŸ“ tests
â”‚   â””â”€â”€ ğŸ“ test_eledubby
â”‚       â”œâ”€â”€ ğŸ“ api
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ test_elevenlabs_client.py
â”‚       â”œâ”€â”€ ğŸ“ audio
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ test_analyzer.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ test_extractor.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ test_processor.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ test_segmenter.py
â”‚       â”œâ”€â”€ ğŸ“ utils
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ test_temp_manager.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ test_cast.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ CLAUDE.md
â”œâ”€â”€ ğŸ“„ example_fx_config.toml
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ publish.sh
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>

__pycache__/
__pypackages__/
.cache
.coverage
.coverage.*
.dmypy.json
.DS_Store
.eggs/
.env
.hypothesis/
.idea/
.installed.cfg
.ipynb_checkpoints
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pdm.toml
.pybuilder/
.pyre/
.pytest_cache/
.Python
.python-version
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.spyderproject
.spyproject
.tox/
.venv
.vscode/
.webassets-cache
*.bak
*.cover
*.egg
*.egg-info/
*.log
*.m4a
*.manifest
*.mo
*.mp3
*.mp4
*.pot
*.py,cover
*.py[cod]
*.sage.py
*.so
*.spec
*.swp
*.temp
*.tmp
*.wav
*~
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
nosetests.xml
output/
parts/
pip-delete-this-directory.txt
pip-log.txt
Pipfile.lock
poetry.lock
profile_default/
sdist/
share/python-wheels/
src/eledubby/_version.py
target/
temp/
Thumbs.db
uv.lock
var/
venv.bak/
venv/
wheels/
external
</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# CHANGELOG.md

## [Unreleased]

### Added
- New `cast` command to synthesize a text file across multiple voices, supporting `--voices_list` and `--voices_path` (with optional per-line `voice_id;api_key`).

### Changed
- `dub` and `fx` commands accept optional `--api_key` to override `ELEVENLABS_API_KEY` (unused for `fx`).

## [0.2.0] - 2025-08-05

### Added
- **VST3 Plugin Support via Pedalboard**
  - Integration with `pedalboard` library for professional audio post-processing
  - Support for VST3 plugins on macOS, Windows, and Linux
  - Automatic plugin path resolution for system-specific directories
  - TOML-based configuration for plugin parameters
  - Chain multiple effects in specified order

- **New `fx` Command**
  - Apply audio effects without dubbing
  - Works with both video and audio input files
  - Extract audio from video with effects processing
  - `--config` argument for custom VST3 configurations
  - Default configuration support

- **Enhanced Audio/Video File Support**
  - `dub` command now accepts audio files (MP3, WAV, FLAC, AAC, etc.)
  - Audio-to-audio dubbing without video processing
  - Flexible output format handling:
    - Video â†’ Video (dubbed)
    - Video â†’ Audio (extract and dub)
    - Audio â†’ Audio (dub)
  - Smart file type detection using MIME types and ffprobe

- **New Command-Line Arguments**
  - `--fx`: Control audio post-processing in `dub` command
    - `0/off/False`: No post-processing
    - `1/on/True`: Use default config
    - Path to TOML: Custom configuration
  - `--seg_min`: Minimum segment duration (default: 10s)
  - `--seg_max`: Maximum segment duration (default: 20s)
  - `--config`: VST3 configuration path for `fx` command

### Changed
- CLI now uses subcommands: `eledubby dub` and `eledubby fx`
- Output paths are now auto-generated based on input type
- ElevenLabs API is optional for `fx` command
- Improved error messages and validation

### Fixed
- `__main__.py` was calling undefined `main` instead of proper functions
- File path tracking (`this_file` comments) throughout codebase

## [0.1.0] - 2025-08-05

### Added
- Initial implementation of adamdubpy voice dubbing tool
- Core functionality:
  - Audio extraction from video files using ffmpeg
  - Intelligent silence detection and segmentation (10-20 second segments)
  - Speech-to-speech conversion using ElevenLabs API
  - Precise timing preservation with padding/cropping
  - Audio normalization and reassembly
  - Video remuxing with converted audio track
- Modular architecture:
  - `audio/` module for extraction, analysis, segmentation, and processing
  - `api/` module for ElevenLabs integration with retry logic
  - `video/` module for remuxing operations
  - `utils/` module for progress tracking and temp file management
- CLI interface using fire with arguments:
  - `--input` (required): Input video path
  - `--voice` (optional): ElevenLabs voice ID
  - `--output` (optional): Output video path
  - `--verbose` (optional): Detailed logging
- Features:
  - Real-time progress tracking with rich console output
  - Comprehensive error handling and recovery
  - Automatic voice ID validation with fallback
  - Disk space checking before processing
  - Detailed processing statistics
- Documentation:
  - Comprehensive README with installation and usage instructions
  - Inline code documentation with docstrings
  - CLAUDE.md for AI assistant guidance

### Technical Details
- Python 3.12+ with uv script runner support
- Dependencies: elevenlabs, python-dotenv, fire, rich, loguru, numpy, scipy
- Requires ffmpeg for audio/video operations
- Supports various video formats (MP4, AVI, MOV, etc.)
- Processes at 2-5x real-time speed typically
</document_content>
</document>

<document index="3">
<source>CLAUDE.md</source>
<document_content>
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## 1. Project Overview

`adamdubpy` is a Python tool for voice dubbing that:
- Takes an input video and replaces the audio with a new voice using ElevenLabs API
- Performs speech-to-speech conversion on audio segments
- Splits audio into 10-20 second segments based on silence detection
- Preserves timing by padding or cropping converted segments

## 2. Required Environment Variables

- `ELEVENLABS_API_KEY` - Must be set for the ElevenLabs API authentication (loaded via python-dotenv)

## 3. Key Dependencies

- **elevenlabs-python**: Main package for speech-to-speech conversion
- **python-dotenv**: For loading environment variables
- **ffmpeg**: Required for video/audio processing (must be installed on system)

## 4. Development Commands

### 4.1. Setup
```bash
# Install dependencies
pip install elevenlabs python-dotenv

# Set up environment
echo "ELEVENLABS_API_KEY=your_api_key_here" > .env
```

### 4.2. Running the Tool
```bash
python adamdubpy.py --input video.mp4 --voice voice_id --output output.mp4
```

Default voice ID: `ELEVENLABS_VOICE_ID` environment variable

## 5. Code Architecture

### 5.1. Audio Processing Pipeline
1. **Audio Extraction**: Extract audio from input video
2. **Silence Detection**: Identify split points based on silence (scoring system for silence duration and length)
3. **Segmentation**: Split audio into 10-20 second segments at optimal silence points
4. **Speech-to-Speech**: Convert each segment using ElevenLabs API with specified voice
5. **Timing Preservation**: Pad or crop converted segments to match original timing
6. **Audio Reassembly**: Combine processed segments
7. **Video Remuxing**: Replace original audio track with processed audio

### 5.2. Key Implementation Details
- Segments must be between 10-20 seconds
- Within each 10-20 second window, find the "most silent long pause"
- Score pauses based on both silence level and duration
- Use ElevenLabs speech-to-speech API for voice conversion
- Maintain exact timing alignment with original video

## 6. Important Notes

- The main script file should be named `adamdubpy.py` (if creating from scratch)
- Reference documentation is available in `ref/` directory but contains very large files
- Always check for ffmpeg availability before processing
- Handle API errors gracefully with retry logic
- Preserve original video quality during remuxing



# Software Development Rules

## 7. Pre-Work Preparation

### 7.1. Before Starting Any Work
- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 7.2. Project Documentation to Maintain
- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 8. General Coding Principles

### 8.1. Core Development Approach
- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 8.2. Code Quality Standards
- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 9. Tool Usage (When Available)

### 9.1. Additional Tools
- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 10. File Management

### 10.1. File Path Tracking
- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 11. Python-Specific Guidelines

### 11.1. PEP Standards
- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 11.2. Modern Python Practices
- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 11.3. CLI Scripts Setup
For CLI Python scripts, use `fire` & `rich`, and start with:
```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 11.4. Post-Edit Python Commands
```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 12. Post-Work Activities

### 12.1. Critical Reflection
- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 12.2. Documentation Updates
- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 13. Work Methodology

### 13.1. Virtual Team Approach
Be creative, diligent, critical, relentless & funny! Lead two experts:
- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 13.2. Continuous Work Mode
- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 14. Special Commands

### 14.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:
- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:
- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 
- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:
- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:
- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**
- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 14.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 14.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 15. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 16. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate
</document_content>
</document>

<document index="4">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</document_content>
</document>

<document index="5">
<source>PLAN.md</source>
<document_content>
# PLAN.md - Eledubby Voice Dubbing Tool

## Project Overview

**Eledubby** is a Python-based voice dubbing and audio processing tool that performs speech-to-speech conversion on video and audio files using ElevenLabs API, with professional audio post-processing capabilities via VST3 plugins.

## Future Enhancements

### Phase 1: Performance and Reliability Improvements

1. **Parallel Processing**

- Implement concurrent segment processing for faster conversion
- Use threading pool for API calls
- Optimize memory usage for large files

2. **Resume Capability**

- Save processing state to checkpoint files
- Allow resuming interrupted conversions
- Implement partial result recovery

3. **Batch Processing**

- Process multiple videos/audio files in sequence
- Support glob patterns for input files
- Generate batch reports

### Phase 2: Advanced Audio Features

1. **Enhanced VST3 Support**

- Auto-discovery of installed VST3 plugins
- Plugin preset management
- Real-time parameter adjustment during processing
- Support for AU (Audio Unit) plugins on macOS

2. **Audio Analysis and Enhancement**

- Automatic volume leveling
- Noise reduction preprocessing
- Dynamic range compression
- Spectral analysis and EQ matching

3. **Advanced Segmentation**

- ML-based scene detection for better split points
- Language-aware segmentation
- Musical phrase detection for music videos

### Phase 3: Multi-Language and Voice Features

1. **Language Support**

- Auto-detect source language
- Language-specific voice selection
- Cross-language dubbing support

2. **Voice Management**

- Voice preview/testing mode
- Custom voice profiles
- Voice blending for multiple speakers
- Speaker diarization for multi-speaker content

3. **Subtitle Integration**

- Generate subtitles from original audio
- Sync subtitles with dubbed audio
- Multi-language subtitle support

### Phase 4: User Experience Enhancements

1. **Preview Mode**

- Process first 30-60 seconds for quick testing
- Side-by-side comparison of original vs dubbed
- Real-time parameter adjustment

2. **Quality Metrics**

- Audio quality assessment scores
- Timing accuracy reports
- Voice similarity metrics
- Processing statistics dashboard

3. **Configuration Management**

- Project-based settings
- Configuration templates
- Import/export settings
- Cloud sync for settings

### Phase 5: Platform and Integration

1. **GUI Application**

- Desktop application with drag-and-drop
- Real-time progress visualization
- Waveform display and editing
- Built-in audio player with A/B comparison

2. **Cloud Processing**

- Upload to cloud for processing
- Distributed processing for long videos
- Web-based interface
- API for third-party integration

3. **Platform Extensions**

- Docker containerization
- GitHub Actions integration
- CI/CD pipeline support
- Plugin architecture for custom processors

### Phase 6: Advanced Video Features

1. **Lip Sync Optimization**

- Analyze video for lip movements
- Adjust audio timing for better sync
- Support for animated content

2. **Scene-Aware Processing**

- Different processing for music vs dialogue
- Ambient sound preservation
- Background music handling

3. **Format Support**

- Additional video formats (WebM, MKV, etc.)
- Streaming format support (HLS, DASH)
- HDR video preservation
- Multi-track audio support

## Technical Debt and Maintenance

### Code Quality

- Add comprehensive unit tests
- Implement integration tests
- Add type hints throughout
- Improve error messages and logging

### Documentation

- API documentation
- Video tutorials
- Example configurations
- Troubleshooting guide

### Performance Optimization

- Profile and optimize bottlenecks
- Reduce memory footprint
- Implement streaming processing
- Cache optimization

## Success Metrics for Future Releases

### Performance Goals

- Process 4K videos efficiently
- Support videos over 2 hours
- Reduce processing time by 50%
- Memory usage under 4GB for 4K videos

### Quality Goals

- 99% audio-video sync accuracy
- Support for 95% of common formats
- Zero quality loss in video
- Natural-sounding voice conversion

### User Experience Goals

- One-click processing for common tasks
- < 5 minute setup time
- Intuitive configuration
- Comprehensive error recovery
</document_content>
</document>

<document index="6">
<source>README.md</source>
<document_content>
# Eledubby

[![PyPI version](https://badge.fury.io/py/eledubby.svg)](https://badge.fury.io/py/eledubby)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

**Eledubby** is a Python tool for automatic voice dubbing of videos using the ElevenLabs API. It performs speech-to-speech conversion, allowing you to replace the original voice in a video with a different voice while preserving timing and synchronization.

## 1. Features

- ğŸ¥ **Automatic voice dubbing** - Replace voices in videos with high-quality AI voices
- ğŸ¯ **Timing preservation** - Maintains original timing by intelligently padding or cropping audio
- ğŸ”Š **Smart audio segmentation** - Splits audio into optimal segments based on silence detection
- ğŸš€ **Batch processing** - Processes multiple segments in parallel for faster results
- ğŸ“Š **Progress tracking** - Real-time progress updates with detailed status information
- ğŸ›¡ï¸ **Error resilience** - Automatic retries and graceful error handling
- ğŸ›ï¸ **Customizable parameters** - Fine-tune silence detection and segmentation settings
- ğŸ¸ **VST3 Plugin Support** - Apply professional audio effects using VST3 plugins via Pedalboard

## 2. Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [How It Works](#how-it-works)
- [Architecture](#architecture)
- [Configuration](#configuration)
- [API Reference](#api-reference)
- [Contributing](#contributing)
- [License](#license)

## 3. Installation

### 3.1. Prerequisites

- Python 3.8 or higher
- [FFmpeg](https://ffmpeg.org/) installed and available in your system PATH
- An [ElevenLabs API key](https://elevenlabs.io/)

### 3.2. Install from PyPI

```bash
pip install eledubby
```

### 3.3. Install from Source

```bash
git clone https://github.com/twardoch/eledubby.git
cd eledubby
pip install -e .
```

### 3.4. Development Installation

```bash
git clone https://github.com/twardoch/eledubby.git
cd eledubby
uv venv
uv sync
```

## 4. Quick Start

1. **Set up your ElevenLabs API key:**

   Create a `.env` file in your project directory:
   ```bash
   echo "ELEVENLABS_API_KEY=your_api_key_here" > .env
   ```

   Or set it as an environment variable:
   ```bash
   export ELEVENLABS_API_KEY=your_api_key_here
   ```

2. **Run eledubby on a video:**

   ```bash
   eledubby --input video.mp4 --voice voice_id --output dubbed_video.mp4
   ```

   To use the default voice:
   ```bash
   eledubby --input video.mp4 --output dubbed_video.mp4
   ```

   With audio post-processing:
   ```bash
   eledubby --input video.mp4 --output dubbed_video.mp4 --fx on
   ```

## 5. Usage

### 5.1. Command Line Interface

The package provides two main commands:

#### 5.1.1. `dub` - Voice Dubbing with ElevenLabs

```bash
eledubby dub [OPTIONS]
```

**Options:**
- `--input` (required): Path to the input video or audio file
- `--output`: Path to the output file (default: auto-generated)
- `--voice`: ElevenLabs voice ID (default: ELEVENLABS_VOICE_ID environment variable)
- `--api_key`: ElevenLabs API key override (default: ELEVENLABS_API_KEY environment variable)
- `--fx`: Audio post-processing effects:
  - `0`, `off`, or `False`: No post-processing
  - `1`, `on`, or `True`: Use default config from `src/eledubby/config.toml`
  - Path to TOML file: Use custom VST3 plugin configuration
- `--seg_min`: Minimum segment duration in seconds (default: 10)
- `--seg_max`: Maximum segment duration in seconds (default: 20)
- `--verbose`: Enable verbose logging

#### 5.1.2. `fx` - Audio Effects Only (No Dubbing)

```bash
eledubby fx [OPTIONS]
```

**Options:**
- `--input` (required): Path to the input video or audio file
- `--output`: Path to the output file (default: auto-generated with same format)
- `--api_key`: ElevenLabs API key override (unused; for interface consistency)
- `--config`: Path to TOML config file for VST3 plugins (default: src/eledubby/config.toml)
- `--verbose`: Enable verbose logging

#### 5.1.3. `cast` - Generate MP3s for Many Voices

```bash
eledubby cast --text_path TEXT.txt [OPTIONS]
```

**Options:**
- `--text_path` (required): Path to the text file to synthesize
- `--output`: Output folder path (default: `basename_without_extension(text_path)` in current directory)
- `--api_key`: ElevenLabs API key override (default: ELEVENLABS_API_KEY environment variable)
- `--voices_list`: Comma-separated voice IDs (e.g. `a,b,c`)
- `--voices_path`: Text file with one voice per line; optionally `voice_id;api_key` per line
- `--verbose`: Enable verbose logging

#### 5.1.4. Examples

**Dubbing Examples:**

Basic video dubbing:
```bash
eledubby dub --input interview.mp4 --voice rachel_voice_id --output interview_dubbed.mp4
```

Audio file dubbing (audio-to-audio):
```bash
eledubby dub --input podcast.mp3 --voice alex_voice_id --output podcast_dubbed.wav
```

With custom segment durations:
```bash
eledubby dub --input podcast.mp4 --output podcast_dubbed.mp4 \
  --seg_min 8 --seg_max 15 --verbose
```

With audio post-processing:
```bash
eledubby dub --input video.mp4 --output enhanced_video.mp4 \
  --fx ./my_effects.toml --verbose
```

**Effects-Only Examples:**

Apply effects to video (no dubbing):
```bash
eledubby fx --input video.mp4 --output enhanced_video.mp4 \
  --config ./voice_enhancement.toml
```

Apply effects to audio file:
```bash
eledubby fx --input recording.wav --output processed_recording.wav
```

Extract and process audio from video:
```bash
eledubby fx --input video.mp4 --output audio_only.wav \
  --config ./mastering.toml
```

**Cast Examples:**

Use all voices for the account:
```bash
eledubby cast --text_path script.txt --api_key YOUR_KEY
```

Use explicit voice IDs:
```bash
eledubby cast --text_path script.txt --voices_list a,b,c --api_key YOUR_KEY
```

Use a voices file (optional per-voice API key):
```bash
eledubby cast --text_path script.txt --voices_path voices.txt
```

### 5.2. Python API

```python
from eledubby.eledubby import dub, fx

# Basic video dubbing
dub(
    input="video.mp4",
    output="dubbed_video.mp4",
    voice="voice_id_here"
)

# Audio file dubbing
dub(
    input="podcast.mp3",
    output="dubbed_podcast.wav",
    voice="voice_id_here"
)

# With audio post-processing
dub(
    input="video.mp4",
    output="enhanced_video.mp4",
    voice="voice_id_here",
    fx=True,  # Use default config
    seg_min=8,
    seg_max=15
)

# Apply effects only (no dubbing)
fx(
    input="video.mp4",
    output="enhanced_video.mp4",
    config="./voice_enhancement.toml",
    verbose=True
)

# Extract audio from video and apply effects
fx(
    input="video.mp4",
    output="extracted_audio.wav",  # Audio output from video input
    config="./mastering.toml"
)
```

### 5.3. VST3 Plugin Configuration

Create a TOML file to configure VST3 plugins for audio post-processing:

```toml
# Example: voice_enhancement.toml

# Compression to even out volume
["Compressor.vst3"]
threshold_db = -20.0
ratio = 4.0
attack_ms = 1.0
release_ms = 100.0

# EQ for voice clarity
["Pro-Q 3.vst3"]
preset = "Vocal Presence"

# Subtle reverb for natural sound
["ValhallaRoom.vst3"]
mix = 0.1
room_size = 0.3

# Limiter to prevent clipping
["Limiter.vst3"]
threshold_db = -0.5
release_ms = 50.0
```

**VST3 Plugin Path Resolution:**
- **macOS**: `~/Library/Audio/Plug-Ins/VST3` and `/Library/Audio/Plug-Ins/VST3`
- **Windows**: `C:\Program Files\Common Files\VST3` and `C:\Program Files (x86)\Common Files\VST3`
- **Linux**: `~/.vst3`, `/usr/lib/vst3`, `/usr/local/lib/vst3`

Plugins are applied in the order specified in the configuration file.

```python
# Example with custom parameters
process_video(
    input_path="video.mp4",
    output_path="dubbed_video.mp4",
    voice_id="voice_id_here",
    silence_threshold=-35,
    min_segment_duration=8,
    max_segment_duration=15,
    model="eleven_multilingual_v2",
    stability=0.6,
    similarity_boost=0.8
)
```

## 6. How It Works

Eledubby uses a sophisticated pipeline to perform voice dubbing while maintaining synchronization:

### 6.1. **Audio Extraction**
   - Extracts audio track from the input video using FFmpeg
   - Preserves original audio format and quality settings

### 6.2. **Silence Detection & Analysis**
   - Analyzes the audio waveform to detect periods of silence
   - Uses configurable threshold (dB) and minimum duration parameters
   - Creates a silence map for intelligent segmentation

### 6.3. **Smart Segmentation**
   - Splits audio into segments between 10-20 seconds (configurable)
   - Finds optimal split points at the longest silence within each window
   - Scores silence periods based on both duration and silence level
   - Ensures segments are within the acceptable duration range

### 6.4. **Speech-to-Speech Conversion**
   - Sends each segment to ElevenLabs API for voice conversion
   - Uses the specified voice ID and model parameters
   - Processes multiple segments in parallel for efficiency

### 6.5. **Timing Preservation**
   - Compares the duration of converted segments with originals
   - Pads shorter segments with silence to match original timing
   - Crops longer segments if necessary (with intelligent trimming)
   - Maintains frame-accurate synchronization

### 6.6. **Audio Reassembly**
   - Concatenates all processed segments in order
   - Ensures seamless transitions between segments
   - Produces a final audio track with exact original duration

### 6.7. **Video Remuxing**
   - Replaces the original audio track with the dubbed version
   - Preserves all video streams and metadata
   - Outputs the final dubbed video file

## 7. Architecture

The project is organized into modular components:

```
eledubby/
â”œâ”€â”€ api/               # ElevenLabs API integration
â”‚   â””â”€â”€ elevenlabs_client.py
â”œâ”€â”€ audio/            # Audio processing modules
â”‚   â”œâ”€â”€ analyzer.py   # Silence detection and analysis
â”‚   â”œâ”€â”€ extractor.py  # Audio extraction from video
â”‚   â”œâ”€â”€ processor.py  # Main audio processing pipeline
â”‚   â””â”€â”€ segmenter.py  # Audio segmentation logic
â”œâ”€â”€ video/            # Video processing modules
â”‚   â””â”€â”€ remuxer.py    # Video remuxing operations
â”œâ”€â”€ utils/            # Utility modules
â”‚   â”œâ”€â”€ progress.py   # Progress tracking
â”‚   â””â”€â”€ temp_manager.py # Temporary file management
â””â”€â”€ adamdubpy.py     # Main CLI entry point
```

### 7.1. Key Components

- **ElevenLabsClient**: Manages API communication with retry logic and error handling
- **AudioAnalyzer**: Performs silence detection using scipy signal processing
- **AudioSegmenter**: Implements the intelligent segmentation algorithm
- **AudioProcessor**: Orchestrates the entire audio processing pipeline
- **VideoRemuxer**: Handles video operations using FFmpeg
- **ProgressTracker**: Provides real-time progress updates using Rich

## 8. Configuration

### 8.1. Environment Variables

- `ELEVENLABS_API_KEY`: Your ElevenLabs API key (required)
- `ELEDUBBY_TEMP_DIR`: Custom temporary directory (optional)
- `ELEDUBBY_MAX_RETRIES`: Maximum API retry attempts (default: 3)
- `ELEDUBBY_RETRY_DELAY`: Delay between retries in seconds (default: 1)

### 8.2. Voice IDs

You can find available voice IDs in your ElevenLabs account or use the API to list them:

```python
from elevenlabs import voices

# List all available voices
for voice in voices():
    print(f"{voice.voice_id}: {voice.name}")
```

### 8.3. Models

Supported ElevenLabs models:
- `eleven_multilingual_v2` (default) - Best quality, supports multiple languages
- `eleven_monolingual_v1` - English only, faster processing
- `eleven_turbo_v2` - Fastest processing, good quality

## 9. API Reference

### 9.1. Main Functions

#### 9.1.1. `process_video()`

```python
def process_video(
    input_path: str,
    output_path: str,
    voice_id: str = os.getenv("ELEVENLABS_VOICE_ID"),
    silence_threshold: float = -40,
    min_silence_duration: float = 0.5,
    min_segment_duration: float = 10,
    max_segment_duration: float = 20,
    padding_duration: float = 0.1,
    model: str = "eleven_multilingual_v2",
    stability: float = 0.5,
    similarity_boost: float = 0.75,
    max_workers: int = 3,
    api_key: Optional[str] = None
) -> None:
    """
    Process a video file by replacing its audio with a dubbed version.
    
    Args:
        input_path: Path to input video file
        output_path: Path to output video file
        voice_id: ElevenLabs voice ID to use
        silence_threshold: Threshold for silence detection in dB
        min_silence_duration: Minimum duration of silence in seconds
        min_segment_duration: Minimum segment duration in seconds
        max_segment_duration: Maximum segment duration in seconds
        padding_duration: Padding to add to segments in seconds
        model: ElevenLabs model to use
        stability: Voice stability parameter (0.0-1.0)
        similarity_boost: Voice similarity boost parameter (0.0-1.0)
        max_workers: Maximum number of parallel workers
        api_key: ElevenLabs API key (uses env var if not provided)
    """
```

### 9.2. Module Classes

#### 9.2.1. `AudioProcessor`

Main class for audio processing operations:

```python
processor = AudioProcessor(
    api_key="your_api_key",
    voice_id="voice_id",
    model="eleven_multilingual_v2",
    max_workers=3
)

# Process audio file
processor.process_audio(
    input_audio_path="audio.wav",
    output_audio_path="dubbed_audio.wav"
)
```

#### 9.2.2. `AudioAnalyzer`

Analyzes audio for silence detection:

```python
analyzer = AudioAnalyzer(
    silence_threshold=-40,
    min_silence_duration=0.5
)

# Detect silence periods
silence_periods = analyzer.detect_silence(audio_data, sample_rate)
```

## 10. Why Eledubby?

### 10.1. The Problem

Traditional dubbing requires voice actors, recording studios, and extensive post-production work. Even with modern AI voice synthesis, maintaining synchronization between video and dubbed audio remains challenging.

### 10.2. The Solution

Eledubby automates the entire dubbing process while solving key synchronization challenges:

1. **Intelligent Segmentation**: Instead of processing the entire audio at once (which can cause drift), Eledubby splits audio at natural pause points.

2. **Timing Preservation**: Each segment is processed individually and adjusted to match the original duration, preventing accumulative timing errors.

3. **Quality Optimization**: By working with smaller segments, the AI voice synthesis produces more consistent and natural results.

4. **Parallel Processing**: Multiple segments are processed simultaneously, significantly reducing total processing time.

### 10.3. Technical Approach

The core innovation is the silence-based segmentation algorithm:

```python
# Pseudocode for segmentation logic
for window in sliding_windows(audio, size=20s, step=10s):
    silence_periods = detect_silence(window)
    best_split = max(silence_periods, key=lambda s: s.duration * s.silence_level)
    segments.append(split_at(audio, best_split))
```

This ensures:
- Natural breaking points that don't cut off speech
- Consistent segment sizes for reliable API processing
- Flexibility to handle various speech patterns

## 11. Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### 11.1. Development Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/twardoch/eledubby.git
   cd eledubby
   ```

2. Create a virtual environment:
   ```bash
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. Install development dependencies:
   ```bash
   uv sync
   ```

4. Run tests:
   ```bash
   pytest
   ```

5. Run linting:
   ```bash
   ruff check .
   mypy .
   ```

### 11.2. Code Style

- Follow PEP 8 guidelines
- Use type hints for all function signatures
- Add docstrings to all public functions and classes
- Write tests for new functionality

## 12. License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 13. Acknowledgments

- [ElevenLabs](https://elevenlabs.io/) for providing the amazing voice synthesis API
- [FFmpeg](https://ffmpeg.org/) for reliable video/audio processing
- [Rich](https://github.com/Textualize/rich) for beautiful terminal output
- The Python community for excellent libraries and tools

## 14. Troubleshooting

### 14.1. Common Issues

1. **FFmpeg not found**
   - Ensure FFmpeg is installed: `ffmpeg -version`
   - Add FFmpeg to your system PATH

2. **API key errors**
   - Verify your API key is correct
   - Check your ElevenLabs account has sufficient credits

3. **Memory issues with large videos**
   - Process videos in smaller chunks
   - Reduce the number of parallel workers
   - Use a machine with more RAM

4. **Audio sync issues**
   - Try adjusting the padding duration
   - Experiment with different segment durations
   - Check that the input video has constant frame rate

### 14.2. Getting Help

- Check the [Issues](https://github.com/twardoch/eledubby/issues) page
- Create a new issue with detailed information about your problem
- Include error messages, system information, and sample files if possible

---

Made with â¤ï¸ by [Adam Twardoch](https://github.com/twardoch)
</document_content>
</document>

<document index="7">
<source>TODO.md</source>
<document_content>
# TODO.md - Eledubby Future Implementation Tasks

## Performance and Reliability

- [ ] Implement parallel segment processing for faster conversion
- [ ] Add threading pool for concurrent API calls
- [ ] Optimize memory usage for large files
- [ ] Create checkpoint system for resume capability
- [ ] Implement partial result recovery
- [ ] Add batch processing for multiple files
- [ ] Support glob patterns for input selection

## Advanced Audio Features

- [ ] Add auto-discovery of installed VST3 plugins
- [ ] Implement plugin preset management
- [ ] Support AU (Audio Unit) plugins on macOS
- [ ] Add automatic volume leveling
- [ ] Implement noise reduction preprocessing
- [ ] Add dynamic range compression
- [ ] Create ML-based scene detection for better splits
- [ ] Implement language-aware segmentation
- [ ] Add musical phrase detection for music videos

## Multi-Language and Voice

- [ ] Implement source language auto-detection
- [ ] Add language-specific voice selection
- [ ] Support cross-language dubbing
- [ ] Create voice preview/testing mode
- [ ] Add custom voice profiles
- [ ] Implement voice blending for multiple speakers
- [ ] Add speaker diarization
- [ ] Generate subtitles from original audio
- [ ] Sync subtitles with dubbed audio

## User Experience

- [ ] Add preview mode (first 30-60 seconds)
- [ ] Create side-by-side comparison view
- [ ] Implement real-time parameter adjustment
- [ ] Add audio quality assessment scores
- [ ] Create timing accuracy reports
- [ ] Implement project-based settings
- [ ] Add configuration templates
- [ ] Create import/export for settings

## GUI and Platform

- [ ] Develop desktop GUI application
- [ ] Add drag-and-drop support
- [ ] Implement waveform visualization
- [ ] Create built-in audio player with A/B comparison
- [ ] Add Docker containerization
- [ ] Create GitHub Actions integration
- [ ] Implement web-based interface
- [ ] Add API for third-party integration

## Advanced Video Features

- [ ] Analyze video for lip sync optimization
- [ ] Implement scene-aware processing
- [ ] Add ambient sound preservation
- [ ] Support background music handling
- [ ] Add support for WebM, MKV formats
- [ ] Implement streaming format support (HLS, DASH)
- [ ] Preserve HDR video
- [ ] Support multi-track audio

## Testing and Quality

- [ ] Write comprehensive unit tests
- [ ] Create integration test suite
- [ ] Add end-to-end testing
- [ ] Implement performance benchmarks
- [ ] Add memory usage monitoring
- [ ] Create automated quality checks
- [ ] Set up CI/CD pipeline

## Documentation

- [ ] Write detailed API documentation
- [ ] Create video tutorials
- [ ] Add more example configurations
- [ ] Write troubleshooting guide
- [ ] Create plugin development guide
- [ ] Add best practices documentation
</document_content>
</document>

<document index="8">
<source>WORK.md</source>
<document_content>
# WORK.md - Current Work Progress

## Latest Work - Enhanced Audio/Video Support and FX Function

### In Progress / Just Done - `cast` command + API key overrides

- Added `cast` CLI command to synthesize a text file to MP3 across multiple voices.
- Added optional `--api_key` to `dub`, `fx`, and `cast` (overrides `ELEVENLABS_API_KEY`).
- `cast --voices_path` supports per-line `voice_id;api_key` for multi-account voice sets.
- Tests: `pytest` (6 tests) passing locally.

### New Features Added

1. **`fx` Function**: Apply audio effects without dubbing
   - Works with both audio and video files
   - Extracts audio from video if needed
   - Can output audio from video input
   - Uses `--config` argument for VST3 configuration

2. **Audio File Support in `dub` Function**:
   - Now accepts audio files as input (MP3, WAV, FLAC, etc.)
   - Audio-to-audio dubbing without video processing
   - Automatic format detection and conversion

3. **Flexible Input/Output Handling**:
   - Automatic detection of file types (audio vs video)
   - Smart output format determination
   - Video â†’ Video, Video â†’ Audio, Audio â†’ Audio conversions
   - Cannot convert Audio â†’ Video (validated)

4. **Enhanced File Type Detection**:
   - MIME type detection
   - Extension-based fallback
   - FFprobe verification for uncertain cases

## Previous Work - Audio Post-Processing with Pedalboard (Issue #301)

Successfully integrated the `pedalboard` library for VST3 plugin support in the eledubby project:

### 1. Added Dependencies
- Added `pedalboard>=0.9.0` for VST3 plugin support
- Added `toml>=0.10.2` for configuration file parsing
- Added `loguru>=0.7.0` to dependencies (was already used but missing)

### 2. Implemented New Command-Line Arguments
- `--fx`: Controls audio post-processing
  - `0/off/False`: No post-processing
  - `1/on/True`: Use default config from `src/eledubby/config.toml`
  - Path to TOML file: Use custom configuration
- `--seg_min`: Minimum segment duration (default: 10 seconds)
- `--seg_max`: Maximum segment duration (default: 20 seconds)

### 3. VST3 Plugin Support Implementation
- Added `_resolve_vst3_path()` method for automatic plugin discovery
- Searches system-specific directories:
  - macOS: `~/Library/Audio/Plug-Ins/VST3` and `/Library/Audio/Plug-Ins/VST3`
  - Windows: `C:\Program Files\Common Files\VST3` and `C:\Program Files (x86)\Common Files\VST3`
  - Linux: `~/.vst3`, `/usr/lib/vst3`, `/usr/local/lib/vst3`
- Supports both absolute paths and plugin names

### 4. Audio Effects Processing
- Added `_apply_audio_fx()` method using pedalboard
- Loads VST3 plugins dynamically from TOML configuration
- Applies plugins in the order specified
- Robust error handling for missing plugins or invalid parameters

### 5. Configuration System
- Created `src/eledubby/config.toml` with commented examples
- Created `example_fx_config.toml` showing practical usage
- TOML format allows easy parameter configuration for each plugin

### 6. Bug Fixes
- Fixed `__main__.py` calling undefined `main` instead of `dub`
- Updated `this_file` paths throughout the codebase

### 7. Documentation Updates
- Updated README with VST3 plugin features
- Added usage examples for new arguments
- Documented plugin configuration format
- Added system-specific plugin path information

## Technical Details

### Audio Processing Pipeline
Post-processing effects are applied:
1. After all segments are converted by ElevenLabs
2. After segments are concatenated
3. After audio normalization
4. Before remuxing with video

### Plugin Loading Process
1. Parse TOML configuration file
2. For each plugin section:
   - Resolve plugin path (absolute or system search)
   - Load VST3 using pedalboard.load_plugin()
   - Set parameters from config
   - Add to processing chain
3. Apply all plugins in sequence to audio

### Error Resilience
- Missing plugins: Log warning, skip plugin
- Invalid parameters: Log warning, use defaults
- No plugins loaded: Use original audio
- Processing failures: Fallback to unprocessed audio

## Usage Examples

### Basic usage with effects:
```bash
eledubby --input video.mp4 --output dubbed.mp4 --fx on
```

### Custom configuration:
```bash
eledubby --input video.mp4 --output dubbed.mp4 --fx ./my_effects.toml
```

### Adjust segment durations:
```bash
eledubby --input video.mp4 --output dubbed.mp4 --seg_min 5 --seg_max 15
```

### Combined options:
```bash
eledubby --input video.mp4 --output pro_dubbed.mp4 \
  --voice custom_voice_id \
  --fx ./voice_enhancement.toml \
  --seg_min 8 --seg_max 18 \
  --verbose
```

## Status
âœ… All requested features from Issue #301 have been implemented and integrated into the codebase.
</document_content>
</document>

<document index="9">
<source>example_fx_config.toml</source>
<document_content>
# Example audio post-processing configuration for eledubby
# This shows how to configure VST3 plugins for voice enhancement

# Apply compression to even out volume levels
["Compressor.vst3"]
threshold_db = -20.0
ratio = 4.0
attack_ms = 1.0
release_ms = 100.0

# Add slight reverb for more natural sound
["Reverb.vst3"]
mix = 0.1  # 10% wet signal
room_size = 0.3
damping = 0.7

# Final limiter to prevent clipping
["Limiter.vst3"]
threshold_db = -0.5
release_ms = 50.0
</document_content>
</document>

<document index="10">
<source>issues/301.txt</source>
<document_content>

Into @src/eledubby and @pyproject.toml incorporate @external/pedalboard.txt and in @src/eledubby/eledubby.py `dub` function add: 

--fx NNN where NNN can be 
    - 0 or `off` or `False` for no post-processing
    - 1 or `on` or `True` for post-processing with default config (which should be in @src/eledubby/config.toml )
    - path to a TOML config file for post-processing
--seg_min for minimal segment length (10 seconds default)
--seg_max for maximal segment length (20 seconds default)

--------------------------------

If `fx` is on (with default or custom config), then after merging the ElevenLabs API results and before putting the audio into video, we process the audio file with the `pedalboard` library, and we apply VST3 plugins. 

In the TOML config file: 

(1) Each section should be a path to a VST3 plugin. 

If the path is an absolute path, we use it. Otherwise we check in the correct system-specific folders: 

- on macOS are `$HOME/Library/Audio/Plug-Ins/VST3` and if the plugin is not found then `/Library/Audio/Plug-Ins/VST3`
- on Windows `C:\Program Files\Common Files\VST3` and if the plugin is not found then `C:\Program Files (x86)\Common Files\VST3` 

(2) Inside the section, we specify the plugin parameters. 

--------------------------------

We apply the plugins in the order they are specified in the TOML config file. 
</document_content>
</document>

<document index="11">
<source>publish.sh</source>
<document_content>
#!/usr/bin/env bash
cd $(dirname "$0")

uvx hatch clean
uvx codetoprompt \
    --compress \
    --output "./llms.txt" \
    --respect-gitignore \
    --cxml \
    --exclude "*.svg,.specstory,ref,testdata,*.lock,llms.txt" \
    "."
gitnextver .
uvx hatch build
uvx hatch publish
</document_content>
</document>

<document index="12">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[project]
name = "eledubby"
dynamic = ["version"]
description = "Voice dubbing tool using ElevenLabs API for speech-to-speech conversion"
readme = "README.md"
license = "MIT"
requires-python = ">=3.12"
authors = [
    { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: End Users/Desktop",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Multimedia :: Sound/Audio :: Speech",
    "Topic :: Multimedia :: Video :: Conversion",
]
dependencies = [
    "elevenlabs>=2.8.1",
    "python-dotenv>=1.0.0",
    "rich>=14.1.0",
    "numpy>=2.3.2",
    "scipy>=1.7.0",
    "fire>=0.7.0",
    "pedalboard>=0.9.0",
    "toml>=0.10.2",
    "loguru>=0.7.0",
]

[project.urls]
Homepage = "https://github.com/twardoch/eledubby"
Documentation = "https://github.com/twardoch/eledubby#readme"
Repository = "https://github.com/twardoch/eledubby"
Issues = "https://github.com/twardoch/eledubby/issues"

[project.scripts]
eledubby = "eledubby.__main__:cli"

[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/eledubby/_version.py"

[tool.hatch.build.targets.sdist]
include = [
    "/src",
    "/README.md",
    "/LICENSE",
]

[tool.hatch.build.targets.wheel]
packages = ["src/eledubby"]

[tool.uv]
dev-dependencies = [
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "pytest>=8.4.1",
    "pytest-cov>=4.0.0",
]

[tool.ruff]
line-length = 100
target-version = "py312"
extend-include = ["*.pyi"]

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # pyflakes
    "I",      # isort
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "UP",     # pyupgrade
    "ARG",    # flake8-unused-arguments
    "SIM",    # flake8-simplify
]
ignore = [
    "E501",   # line too long
    "B008",   # do not perform function calls in argument defaults
    "W191",   # indentation contains tabs
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.lint.isort]
known-first-party = ["eledubby"]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = [
    "elevenlabs.*",
    "scipy.*",
    "numpy.*",
]
ignore_missing_imports = true
</document_content>
</document>

# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/__init__.py
# Language: python

from ._version import __version__, __version_tuple__


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/__main__.py
# Language: python

import sys
import fire
from eledubby.eledubby import cast, dub, fx

def cli(()):


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/api/__init__.py
# Language: python

from .elevenlabs_client import ElevenLabsClient


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/api/elevenlabs_client.py
# Language: python

import os
import time
from elevenlabs import ElevenLabs
from elevenlabs.errors import (
    BadRequestError,
    ForbiddenError,
    NotFoundError,
    UnprocessableEntityError,
)
from loguru import logger

class ElevenLabsClient:
    def __init__((self, api_key: str | None = None, max_retries: int = 3)):
    def text_to_speech((
        self,
        *,
        text: str,
        voice_id: str,
        output_path: str,
        output_format: str = "mp3_44100_128",
    )) -> str:
    def speech_to_speech((
        self,
        audio_path: str,
        voice_id: str,
        output_path: str,
        model_id: str = "eleven_english_sts_v2",
    )) -> str:
    def validate_voice_id((self, voice_id: str)) -> bool:
    def list_voices((self)) -> list:

def __init__((self, api_key: str | None = None, max_retries: int = 3)):

def text_to_speech((
        self,
        *,
        text: str,
        voice_id: str,
        output_path: str,
        output_format: str = "mp3_44100_128",
    )) -> str:

def speech_to_speech((
        self,
        audio_path: str,
        voice_id: str,
        output_path: str,
        model_id: str = "eleven_english_sts_v2",
    )) -> str:

def validate_voice_id((self, voice_id: str)) -> bool:

def list_voices((self)) -> list:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/audio/__init__.py
# Language: python

from .analyzer import SilenceAnalyzer
from .extractor import AudioExtractor
from .processor import AudioProcessor
from .segmenter import AudioSegmenter


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/audio/analyzer.py
# Language: python

import numpy as np
from loguru import logger
from scipy.io import wavfile

class SilenceAnalyzer:
    def __init__((self, silence_threshold_db: float = -40)):
    def analyze((
        self, audio_path: str, min_duration: float = 10.0, max_duration: float = 20.0
    )) -> list[tuple[float, float]]:
    def _calculate_silence_scores((
        self, audio_data: np.ndarray, sample_rate: int
    )) -> list[tuple[float, float]]:
    def _find_optimal_segments((
        self,
        silence_scores: list[tuple[float, float]],
        sample_rate: int,  # noqa: ARG002
        min_duration: float,
        max_duration: float,
        total_duration: float,
    )) -> list[tuple[float, float]]:

def __init__((self, silence_threshold_db: float = -40)):

def analyze((
        self, audio_path: str, min_duration: float = 10.0, max_duration: float = 20.0
    )) -> list[tuple[float, float]]:

def _calculate_silence_scores((
        self, audio_data: np.ndarray, sample_rate: int
    )) -> list[tuple[float, float]]:

def _find_optimal_segments((
        self,
        silence_scores: list[tuple[float, float]],
        sample_rate: int,  # noqa: ARG002
        min_duration: float,
        max_duration: float,
        total_duration: float,
    )) -> list[tuple[float, float]]:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/audio/extractor.py
# Language: python

import subprocess
from loguru import logger

class AudioExtractor:
    def __init__((self, sample_rate: int = 16000)):
    def extract((self, video_path: str, output_path: str)) -> str:
    def _get_duration((self, audio_path: str)) -> float:

def __init__((self, sample_rate: int = 16000)):

def extract((self, video_path: str, output_path: str)) -> str:

def _get_duration((self, audio_path: str)) -> float:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/audio/processor.py
# Language: python

import os
import subprocess
from loguru import logger

class AudioProcessor:
    def measure_duration((self, audio_path: str)) -> float:
    def adjust_duration((self, audio_path: str, target_duration: float, output_path: str)) -> str:
    def _pad_audio((self, audio_path: str, pad_duration: float, output_path: str)) -> str:
    def _trim_audio((self, audio_path: str, target_duration: float, output_path: str)) -> str:
    def normalize_audio((self, audio_path: str, output_path: str, target_db: float = -23.0)) -> str:

def measure_duration((self, audio_path: str)) -> float:

def adjust_duration((self, audio_path: str, target_duration: float, output_path: str)) -> str:

def _pad_audio((self, audio_path: str, pad_duration: float, output_path: str)) -> str:

def _trim_audio((self, audio_path: str, target_duration: float, output_path: str)) -> str:

def normalize_audio((self, audio_path: str, output_path: str, target_db: float = -23.0)) -> str:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/audio/segmenter.py
# Language: python

import os
import subprocess
from loguru import logger

class AudioSegmenter:
    def segment((
        self, audio_path: str, segments: list[tuple[float, float]], output_dir: str
    )) -> list[str]:
    def concatenate((self, segment_paths: list[str], output_path: str)) -> str:

def segment((
        self, audio_path: str, segments: list[tuple[float, float]], output_dir: str
    )) -> list[str]:

def concatenate((self, segment_paths: list[str], output_path: str)) -> str:


<document index="13">
<source>src/eledubby/config.toml</source>
<document_content>
# this_file: src/eledubby/config.toml
# Default audio post-processing configuration for eledubby
# Each section represents a VST3 plugin to apply in order

# Example 1: Compression to even out volume levels
# Uncomment to enable compression
# ["Compressor.vst3"]
# threshold_db = -20.0
# ratio = 4.0
# attack_ms = 1.0
# release_ms = 100.0

# Example 2: EQ to adjust frequency response
# Uncomment to enable EQ
# ["FabFilter Pro-Q 3.vst3"]
# preset = "Vocal Presence"

# Example 3: Reverb for spatial enhancement
# Uncomment to enable reverb
# ["ValhallaRoom.vst3"]
# mix = 0.15
# size = 0.5
# decay = 1.2

# Example 4: Limiter to prevent clipping
# Uncomment to enable limiting
# ["FabFilter Pro-L 2.vst3"]
# gain_db = 0.0
# release_ms = 50.0
# lookahead_ms = 1.5

# Note: Use absolute paths or plugin names that will be resolved in system VST3 folders:
# macOS: ~/Library/Audio/Plug-Ins/VST3 and /Library/Audio/Plug-Ins/VST3
# Windows: C:\Program Files\Common Files\VST3 and C:\Program Files (x86)\Common Files\VST3
</document_content>
</document>

# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/eledubby.py
# Language: python

import mimetypes
import os
import platform
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import pedalboard
import toml
from dotenv import load_dotenv
from loguru import logger
from rich.console import Console
from .api import ElevenLabsClient
from .audio import AudioExtractor, AudioProcessor, AudioSegmenter, SilenceAnalyzer
from .utils import ProgressTracker, TempFileManager
from .video import VideoRemuxer
import shutil
import shutil

DEFAULT_VOICE_ID = =
DEFAULT_MIN_SEGMENT_DURATION = =
DEFAULT_MAX_SEGMENT_DURATION = =
SILENCE_THRESHOLD_DB = =
SAMPLE_RATE = =
DEFAULT_CONFIG_PATH = =

class EleDubby:
    def __init__((
        self,
        verbose: bool = False,
        seg_min: float = DEFAULT_MIN_SEGMENT_DURATION,
        seg_max: float = DEFAULT_MAX_SEGMENT_DURATION,
        require_elevenlabs: bool = True,
        api_key: str | None = None,
        parallel: int = 1,
        preview: float = 0,
    )):
    def _setup_logging((self)):
    def _check_dependencies((self, require_elevenlabs: bool = True)):
    def _resolve_vst3_path((self, plugin_path: str)) -> str | None:
    def _apply_audio_fx((self, audio_path: str, fx_config: str | dict, output_path: str)) -> str:
    def _is_video_file((self, file_path: str)) -> bool:
    def _determine_output_format((
        self,
        input_path: str,
        output_path: str | None,
        is_input_video: bool,
        force_audio: bool = False,
    )) -> tuple[str, bool]:
    def process((
        self,
        input: str,
        voice: str = DEFAULT_VOICE_ID,
        output: str | None = None,
        fx: str | bool | None = None,
    )):

def __init__((
        self,
        verbose: bool = False,
        seg_min: float = DEFAULT_MIN_SEGMENT_DURATION,
        seg_max: float = DEFAULT_MAX_SEGMENT_DURATION,
        require_elevenlabs: bool = True,
        api_key: str | None = None,
        parallel: int = 1,
        preview: float = 0,
    )):

def _setup_logging((self)):

def _check_dependencies((self, require_elevenlabs: bool = True)):

def _resolve_vst3_path((self, plugin_path: str)) -> str | None:

def _apply_audio_fx((self, audio_path: str, fx_config: str | dict, output_path: str)) -> str:

def _is_video_file((self, file_path: str)) -> bool:

def _determine_output_format((
        self,
        input_path: str,
        output_path: str | None,
        is_input_video: bool,
        force_audio: bool = False,
    )) -> tuple[str, bool]:

def process((
        self,
        input: str,
        voice: str = DEFAULT_VOICE_ID,
        output: str | None = None,
        fx: str | bool | None = None,
    )):

def process_segment((args: tuple[int, str, float, float])) -> tuple[int, str]:

def dub((
    input: str | Path,
    voice: str = DEFAULT_VOICE_ID,
    output: str | Path | None = None,
    api_key: str | None = None,
    verbose: bool = False,
    fx: str | bool | None = None,
    seg_min: float = DEFAULT_MIN_SEGMENT_DURATION,
    seg_max: float = DEFAULT_MAX_SEGMENT_DURATION,
    parallel: int = 1,
    preview: float = 0,
)):

def fx((
    input: str | Path,
    output: str | Path | None = None,
    api_key: str | None = None,
    config: str | None = None,
    verbose: bool = False,
)):

def _parse_voice_specs_from_voices_list((voices_list: str)) -> list[tuple[str, str | None]]:

def _parse_voice_specs_from_voices_path((voices_path: str | Path)) -> list[tuple[str, str | None]]:

def _default_cast_output_dir((text_path: str | Path, output: str | Path | None)) -> Path:

def _safe_filename_component((value: str)) -> str:

def _unique_output_path((path: Path)) -> Path:

def cast((
    text_path: str | Path,
    *,
    output: str | Path | None = None,
    voices_path: str | Path | None = None,
    voices_list: str | None = None,
    api_key: str | None = None,
    verbose: bool = False,
)) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/utils/__init__.py
# Language: python

from .progress import ProgressTracker
from .temp_manager import TempFileManager


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/utils/progress.py
# Language: python

from contextlib import contextmanager
from rich.console import Console
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeRemainingColumn,
)

class ProgressTracker:
    def __init__((self)):
    def print_summary((self, stats: dict)):

def __init__((self)):

def track_segments((self, total: int, description: str = "Processing segments")):

def update((advance: int = 1, description: str | None = None)):

def track_file_operation((self, description: str)):

def print_summary((self, stats: dict)):


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/utils/temp_manager.py
# Language: python

import os
import shutil
import tempfile
from contextlib import contextmanager
from loguru import logger

class TempFileManager:
    def __init__((self, prefix: str = "adamdubpy_")):
    def cleanup_directory((self, directory: str)):
    def cleanup_all((self)):
    def get_temp_path((self, directory: str, filename: str)) -> str:

def __init__((self, prefix: str = "adamdubpy_")):

def temp_directory((self, cleanup: bool = True)):

def cleanup_directory((self, directory: str)):

def cleanup_all((self)):

def get_temp_path((self, directory: str, filename: str)) -> str:

def estimate_space_needed((video_path: str)) -> float:

def check_disk_space((path: str, required_gb: float)) -> bool:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/video/__init__.py
# Language: python

from .remuxer import VideoRemuxer


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/src/eledubby/video/remuxer.py
# Language: python

import subprocess
from loguru import logger
import json

class VideoRemuxer:
    def remux((
        self, video_path: str, audio_path: str, output_path: str, copy_video: bool = True
    )) -> str:
    def _verify_output((self, output_path: str, original_path: str)) -> bool:
    def _get_duration((self, video_path: str)) -> float:
    def extract_metadata((self, video_path: str)) -> dict:

def remux((
        self, video_path: str, audio_path: str, output_path: str, copy_video: bool = True
    )) -> str:

def _verify_output((self, output_path: str, original_path: str)) -> bool:

def _get_duration((self, video_path: str)) -> float:

def extract_metadata((self, video_path: str)) -> dict:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/__init__.py
# Language: python



# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/api/__init__.py
# Language: python



# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/api/test_elevenlabs_client.py
# Language: python

from pathlib import Path
from unittest.mock import MagicMock, patch
import pytest
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient
from eledubby.api import ElevenLabsClient

PATCH_BASE = =

class TestElevenLabsClientInit:
    def test_init_when_api_key_provided_then_uses_it((self)) -> None:
    def test_init_when_no_key_and_no_env_then_raises((self)) -> None:
    def test_init_when_env_key_then_uses_env((self)) -> None:
    def test_init_when_default_retries_then_three((self)) -> None:
    def test_init_when_custom_retries_then_uses_it((self)) -> None:

class TestElevenLabsClientTextToSpeech:
    def test_text_to_speech_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:
    def test_text_to_speech_when_empty_chunks_then_skips_them((
        self, tmp_path: Path
    )) -> None:

class TestElevenLabsClientSpeechToSpeech:
    def test_speech_to_speech_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:

class TestElevenLabsClientValidateVoice:
    def test_validate_voice_id_when_exists_then_returns_true((self)) -> None:
    def test_validate_voice_id_when_not_exists_then_returns_false((self)) -> None:
    def test_validate_voice_id_when_api_error_then_returns_false((self)) -> None:

class TestElevenLabsClientListVoices:
    def test_list_voices_when_success_then_returns_list((self)) -> None:
    def test_list_voices_when_api_error_then_returns_empty((self)) -> None:
    def test_list_voices_when_no_category_then_uses_unknown((self)) -> None:

def test_init_when_api_key_provided_then_uses_it((self)) -> None:

def test_init_when_no_key_and_no_env_then_raises((self)) -> None:

def test_init_when_env_key_then_uses_env((self)) -> None:

def test_init_when_default_retries_then_three((self)) -> None:

def test_init_when_custom_retries_then_uses_it((self)) -> None:

def test_text_to_speech_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:

def test_text_to_speech_when_empty_chunks_then_skips_them((
        self, tmp_path: Path
    )) -> None:

def test_speech_to_speech_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:

def test_validate_voice_id_when_exists_then_returns_true((self)) -> None:

def test_validate_voice_id_when_not_exists_then_returns_false((self)) -> None:

def test_validate_voice_id_when_api_error_then_returns_false((self)) -> None:

def test_list_voices_when_success_then_returns_list((self)) -> None:

def test_list_voices_when_api_error_then_returns_empty((self)) -> None:

def test_list_voices_when_no_category_then_uses_unknown((self)) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/audio/__init__.py
# Language: python



# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/audio/test_analyzer.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import numpy as np
import pytest
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer
from scipy.io import wavfile
from eledubby.audio.analyzer import SilenceAnalyzer
from eledubby.audio.analyzer import SilenceAnalyzer

class TestSilenceAnalyzer:
    def test_init_when_default_then_minus_40_db((self)) -> None:
    def test_init_when_custom_threshold_then_uses_it((self)) -> None:
    def test_calculate_silence_scores_when_silent_audio_then_high_scores((self)) -> None:
    def test_calculate_silence_scores_when_loud_audio_then_low_scores((self)) -> None:
    def test_calculate_silence_scores_when_mixed_audio_then_varied_scores((self)) -> None:
    def test_find_optimal_segments_when_short_audio_then_single_segment((self)) -> None:
    def test_find_optimal_segments_when_long_audio_then_multiple_segments((self)) -> None:
    def test_find_optimal_segments_when_prefers_silent_points_then_splits_at_silence((
        self,
    )) -> None:
    def test_analyze_when_wav_file_then_returns_segments((self, tmp_path: Path)) -> None:

class TestSilenceAnalyzerEdgeCases:
    def test_analyze_when_empty_scores_then_returns_full_segment((self)) -> None:
    def test_calculate_silence_scores_when_all_zeros_normalized_then_handles_gracefully((
        self,
    )) -> None:

def test_init_when_default_then_minus_40_db((self)) -> None:

def test_init_when_custom_threshold_then_uses_it((self)) -> None:

def test_calculate_silence_scores_when_silent_audio_then_high_scores((self)) -> None:

def test_calculate_silence_scores_when_loud_audio_then_low_scores((self)) -> None:

def test_calculate_silence_scores_when_mixed_audio_then_varied_scores((self)) -> None:

def test_find_optimal_segments_when_short_audio_then_single_segment((self)) -> None:

def test_find_optimal_segments_when_long_audio_then_multiple_segments((self)) -> None:

def test_find_optimal_segments_when_prefers_silent_points_then_splits_at_silence((
        self,
    )) -> None:

def test_analyze_when_wav_file_then_returns_segments((self, tmp_path: Path)) -> None:

def test_analyze_when_empty_scores_then_returns_full_segment((self)) -> None:

def test_calculate_silence_scores_when_all_zeros_normalized_then_handles_gracefully((
        self,
    )) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/audio/test_extractor.py
# Language: python

from pathlib import Path
from unittest.mock import MagicMock, patch
import pytest
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor
from eledubby.audio.extractor import AudioExtractor

class TestAudioExtractor:
    def test_init_when_default_then_16000_sample_rate((self)) -> None:
    def test_init_when_custom_rate_then_uses_it((self)) -> None:
    def test_extract_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:
    def test_extract_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:
    def test_extract_when_called_then_uses_correct_ffmpeg_args((
        self, tmp_path: Path
    )) -> None:

class TestAudioExtractorGetDuration:
    def test_get_duration_when_valid_then_returns_float((self)) -> None:
    def test_get_duration_when_ffprobe_fails_then_returns_zero((self)) -> None:
    def test_get_duration_when_invalid_output_then_returns_zero((self)) -> None:
    def test_get_duration_when_empty_output_then_returns_zero((self)) -> None:

def test_init_when_default_then_16000_sample_rate((self)) -> None:

def test_init_when_custom_rate_then_uses_it((self)) -> None:

def test_extract_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:

def test_extract_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:

def test_extract_when_called_then_uses_correct_ffmpeg_args((
        self, tmp_path: Path
    )) -> None:

def test_get_duration_when_valid_then_returns_float((self)) -> None:

def test_get_duration_when_ffprobe_fails_then_returns_zero((self)) -> None:

def test_get_duration_when_invalid_output_then_returns_zero((self)) -> None:

def test_get_duration_when_empty_output_then_returns_zero((self)) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/audio/test_processor.py
# Language: python

from pathlib import Path
from unittest.mock import MagicMock, patch
import pytest
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor
from eledubby.audio.processor import AudioProcessor

class TestAudioProcessorMeasureDuration:
    def test_measure_duration_when_valid_audio_then_returns_duration((self)) -> None:
    def test_measure_duration_when_ffprobe_fails_then_returns_zero((self)) -> None:
    def test_measure_duration_when_invalid_output_then_returns_zero((self)) -> None:

class TestAudioProcessorAdjustDuration:
    def test_adjust_duration_when_within_tolerance_then_copies_file((
        self, tmp_path: Path
    )) -> None:
    def test_adjust_duration_when_needs_padding_then_pads((self, tmp_path: Path)) -> None:
    def test_adjust_duration_when_needs_trimming_then_trims((self, tmp_path: Path)) -> None:

class TestAudioProcessorPadAudio:
    def test_pad_audio_when_called_then_generates_silence_and_concatenates((
        self, tmp_path: Path
    )) -> None:

class TestAudioProcessorTrimAudio:
    def test_trim_audio_when_called_then_uses_ffmpeg_with_duration((
        self, tmp_path: Path
    )) -> None:

class TestAudioProcessorNormalizeAudio:
    def test_normalize_audio_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:
    def test_normalize_audio_when_fails_then_copies_original((
        self, tmp_path: Path
    )) -> None:
    def test_normalize_audio_when_custom_target_then_uses_it((
        self, tmp_path: Path
    )) -> None:

def test_measure_duration_when_valid_audio_then_returns_duration((self)) -> None:

def test_measure_duration_when_ffprobe_fails_then_returns_zero((self)) -> None:

def test_measure_duration_when_invalid_output_then_returns_zero((self)) -> None:

def test_adjust_duration_when_within_tolerance_then_copies_file((
        self, tmp_path: Path
    )) -> None:

def test_adjust_duration_when_needs_padding_then_pads((self, tmp_path: Path)) -> None:

def test_adjust_duration_when_needs_trimming_then_trims((self, tmp_path: Path)) -> None:

def test_pad_audio_when_called_then_generates_silence_and_concatenates((
        self, tmp_path: Path
    )) -> None:

def test_trim_audio_when_called_then_uses_ffmpeg_with_duration((
        self, tmp_path: Path
    )) -> None:

def test_normalize_audio_when_success_then_returns_output_path((
        self, tmp_path: Path
    )) -> None:

def test_normalize_audio_when_fails_then_copies_original((
        self, tmp_path: Path
    )) -> None:

def test_normalize_audio_when_custom_target_then_uses_it((
        self, tmp_path: Path
    )) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/audio/test_segmenter.py
# Language: python

import os
from pathlib import Path
from unittest.mock import MagicMock, patch
import pytest
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter
from eledubby.audio.segmenter import AudioSegmenter

class TestAudioSegmenter:
    def test_segment_when_single_segment_then_creates_one_file((
        self, tmp_path: Path
    )) -> None:
    def test_segment_when_multiple_segments_then_creates_multiple_files((
        self, tmp_path: Path
    )) -> None:
    def test_segment_when_creates_output_directory((self, tmp_path: Path)) -> None:
    def test_segment_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:
    def test_segment_when_calls_ffmpeg_with_correct_args((self, tmp_path: Path)) -> None:

class TestAudioSegmenterConcatenate:
    def test_concatenate_when_multiple_files_then_creates_concat_file((
        self, tmp_path: Path
    )) -> None:
    def test_concatenate_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:
    def test_concatenate_when_single_file_then_still_works((self, tmp_path: Path)) -> None:

def test_segment_when_single_segment_then_creates_one_file((
        self, tmp_path: Path
    )) -> None:

def test_segment_when_multiple_segments_then_creates_multiple_files((
        self, tmp_path: Path
    )) -> None:

def test_segment_when_creates_output_directory((self, tmp_path: Path)) -> None:

def test_segment_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:

def test_segment_when_calls_ffmpeg_with_correct_args((self, tmp_path: Path)) -> None:

def test_concatenate_when_multiple_files_then_creates_concat_file((
        self, tmp_path: Path
    )) -> None:

def test_concatenate_when_ffmpeg_fails_then_raises_runtime_error((
        self, tmp_path: Path
    )) -> None:

def test_concatenate_when_single_file_then_still_works((self, tmp_path: Path)) -> None:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/test_cast.py
# Language: python

from pathlib import Path
import pytest
import eledubby.eledubby as mod
import eledubby.eledubby as mod
import eledubby.eledubby as mod
import eledubby.eledubby as mod
import eledubby.eledubby as mod
import eledubby.eledubby as mod

class FakeClient:
    def __init__((self, api_key: str | None = None, max_retries: int = 3)):
    def text_to_speech((
            self,
            *,
            text: str,
            voice_id: str,
            output_path: str,
            _output_format: str = "mp3_44100_128",
        )) -> str:
    def list_voices((self)) -> list[dict[str, str]]:

class FakeClient:
    def __init__((self, api_key: str | None = None, max_retries: int = 3)):
    def text_to_speech((
            self,
            *,
            text: str,
            voice_id: str,
            output_path: str,
            _output_format: str = "mp3_44100_128",
        )) -> str:
    def list_voices((self)) -> list[dict[str, str]]:

def test_cast_missing_function_then_fails(()) -> None:

def test_parse_voices_list_when_csv_then_parses(()) -> None:

def test_parse_voices_path_when_semicolon_then_parses((tmp_path: Path)) -> None:

def test_default_cast_output_dir_when_output_none_then_basename_in_cwd((
    tmp_path: Path, monkeypatch: pytest.MonkeyPatch
)) -> None:

def test_cast_when_voices_list_then_writes_mp3s((
    tmp_path: Path, monkeypatch: pytest.MonkeyPatch
)) -> None:

def __init__((self, api_key: str | None = None, max_retries: int = 3)):

def text_to_speech((
            self,
            *,
            text: str,
            voice_id: str,
            output_path: str,
            _output_format: str = "mp3_44100_128",
        )) -> str:

def list_voices((self)) -> list[dict[str, str]]:

def test_cast_when_voices_path_with_per_voice_keys_then_uses_keys((
    tmp_path: Path, monkeypatch: pytest.MonkeyPatch
)) -> None:

def __init__((self, api_key: str | None = None, max_retries: int = 3)):

def text_to_speech((
            self,
            *,
            text: str,
            voice_id: str,
            output_path: str,
            _output_format: str = "mp3_44100_128",
        )) -> str:

def list_voices((self)) -> list[dict[str, str]]:


# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/utils/__init__.py
# Language: python



# File: /Users/adam/Developer/github2/twardoch/pub/eledubby/tests/test_eledubby/utils/test_temp_manager.py
# Language: python

import os
from pathlib import Path
from unittest.mock import patch
import pytest
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager
from eledubby.utils.temp_manager import TempFileManager

class TestTempFileManager:
    def test_init_when_default_then_uses_adamdubpy_prefix((self)) -> None:
    def test_init_when_custom_prefix_then_uses_it((self)) -> None:
    def test_temp_directory_when_cleanup_true_then_removes_dir((self)) -> None:
    def test_temp_directory_when_cleanup_false_then_keeps_dir((self)) -> None:
    def test_temp_directory_when_tracks_directories((self)) -> None:
    def test_cleanup_directory_when_exists_then_removes((self, tmp_path: Path)) -> None:
    def test_cleanup_directory_when_not_exists_then_no_error((self)) -> None:
    def test_cleanup_all_when_multiple_dirs_then_cleans_all((self)) -> None:
    def test_get_temp_path_when_called_then_joins_correctly((self)) -> None:

class TestTempFileManagerEstimateSpace:
    def test_estimate_space_needed_when_valid_file_then_returns_estimate((
        self, tmp_path: Path
    )) -> None:
    def test_estimate_space_needed_when_file_not_exists_then_returns_default((
        self,
    )) -> None:

class TestTempFileManagerCheckDiskSpace:
    def test_check_disk_space_when_enough_space_then_returns_true((
        self, tmp_path: Path
    )) -> None:
    def test_check_disk_space_when_too_much_requested_then_returns_false((
        self, tmp_path: Path
    )) -> None:
    def test_check_disk_space_when_error_then_returns_true((self)) -> None:

def test_init_when_default_then_uses_adamdubpy_prefix((self)) -> None:

def test_init_when_custom_prefix_then_uses_it((self)) -> None:

def test_temp_directory_when_cleanup_true_then_removes_dir((self)) -> None:

def test_temp_directory_when_cleanup_false_then_keeps_dir((self)) -> None:

def test_temp_directory_when_tracks_directories((self)) -> None:

def test_cleanup_directory_when_exists_then_removes((self, tmp_path: Path)) -> None:

def test_cleanup_directory_when_not_exists_then_no_error((self)) -> None:

def test_cleanup_all_when_multiple_dirs_then_cleans_all((self)) -> None:

def test_get_temp_path_when_called_then_joins_correctly((self)) -> None:

def test_estimate_space_needed_when_valid_file_then_returns_estimate((
        self, tmp_path: Path
    )) -> None:

def test_estimate_space_needed_when_file_not_exists_then_returns_default((
        self,
    )) -> None:

def test_check_disk_space_when_enough_space_then_returns_true((
        self, tmp_path: Path
    )) -> None:

def test_check_disk_space_when_too_much_requested_then_returns_false((
        self, tmp_path: Path
    )) -> None:

def test_check_disk_space_when_error_then_returns_true((self)) -> None:


</documents>